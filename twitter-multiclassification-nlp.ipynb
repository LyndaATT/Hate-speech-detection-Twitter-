{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow --upgrade\n!pip install openpyxl\n!pip install PyArabic\n!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git &> /dev/null\n!pip install emoji \n!pip install Arabic-Stopwords\n!pip install tkseem\n!pip install tnkeeh\n!pip3 install fr-word-segment\n!pip install pyspellchecker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\n\nimport nltk\nimport string\nfrom french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\nfrom fastai.text.all import *\n\nimport sklearn\nimport regex as reg\nfrom unicodedata import normalize\nfrom collections import Counter\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.data_utils import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import CyclicLR\nfrom torchvision import models\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score,recall_score,f1_score\nimport os\nimport gensim\n\n\n# keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import GRU,MaxPooling1D,GlobalMaxPooling1D,GlobalAveragePooling1D,Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras import callbacks\nfrom keras.utils.vis_utils import plot_model\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer \nfrom sklearn.metrics import roc_auc_score, accuracy_score,roc_curve, auc, plot_confusion_matrix, confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.manifold import TSNE\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.preprocessing.text import Tokenizer\nimport emoji\n\nfrom keras.models import Model\nfrom imblearn.over_sampling import SMOTE\nimport seaborn as sn\nimport pyarabic.araby as ar\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport seaborn as sn\nimport tkseem as tk\nimport tnkeeh as tn\nfrom nltk.stem.isri import ISRIStemmer\nfrom spellchecker import SpellChecker\nfrom wordsegment import load,segment\nfrom keras.layers import Concatenate\nimport tensorflow as tf\nfrom sklearn.utils import resample\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\nfrom fastai.text.all import *\nload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"df_ar = pd.read_csv('/kaggle/input/twitter/ar_dataset.csv')\ndf_fr = pd.read_csv('/kaggle/input/twitter/fr_dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing data","metadata":{}},{"cell_type":"code","source":"#### Class distribution \ncum = df_fr['target'].value_counts().to_frame()\ncum['HITId'] = cum.index\ncumfig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel('my x label', size = 13)\nax.set_ylabel('my y label', size = 13)\nax.set_xticklabels(\"\",size = 13)\nsn.barplot(data=cum,x='HITId',y='target',palette = \"rocket\",ax=ax).set(title='Data distribution respect to class for French language') \nax.set_xlabel('Category', size = 13)\nplt.savefig('Data distribution for French language') \n\n#### Class distribution \ncum = df_ar['target'].value_counts().to_frame()\ncum['HITId'] = cum.index\ncumfig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel('my x label', size = 13)\nax.set_ylabel('my y label', size = 13)\nax.set_xticklabels(\"\",size = 13)\nsn.barplot(data=cum,x='HITId',y='target',palette = \"rocket\",ax=ax).set(title='Data distribution respect to class for Arabic language') \nax.set_xlabel('Category', size = 13)\nplt.savefig('Data distribution for Arabic language') \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.set(font_scale=1)\nprint('French')\nsn.catplot(x=\"directness\", col=\"target\", col_wrap=3,\n                data=df_fr[df_fr.target.notnull()],\n                kind=\"count\", height=4.5,palette = \"rocket\", aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Arabic')\nsn.catplot(x=\"directness\", col=\"target\", col_wrap=3,\n                data=df_ar[df_ar.target.notnull()],\n                kind=\"count\", height=4.5,palette = \"rocket\", aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sn.set(font_scale=1)\nprint('French')\ng = sn.catplot(x=\"group\", col=\"target\", col_wrap=3,\n                data=df_fr[df_fr.target.notnull()],\n                kind=\"count\", height=4,palette = \"rocket\", aspect=.8)\ng.set_xticklabels(rotation=60) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Arabic')\ng= sn.catplot(x=\"group\", col=\"target\", col_wrap=3,\n                data=df_ar[df_ar.target.notnull()],\n                kind=\"count\", height=4.5,palette = \"rocket\", aspect=.8)\ng.set_xticklabels(rotation=60) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing data","metadata":{}},{"cell_type":"code","source":"def hash_fix(h):\n    h1 = re.sub(r'[0-9]+', '', h)\n    h2 = re.sub(r'#', '', h1)\n    h3 = segment(str(h2))\n    h4 = ' '.join(map(str, h3)) \n    return h4\ntok_ar = tk.WordTokenizer()\ntok_ar.train('/kaggle/input/twitter/ar_dataset.csv')\ndef prepro_ar(tweet):\n    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    \n    tweet = tweet.replace('user', '')\n    tweet = tweet.replace('@user', '')\n    tweet = re.sub( r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",'',tweet)\n\n    tweet = tweet.replace('url', '')\n    tweet = re.sub(r'\\u003c\\\\1', lambda m: \"\", tweet)\n    tweet = re.sub(arabic_diacritics, '', str(tweet))\n    tweet = re.sub(r'(.)\\1+', \"\", tweet) \n    tweet = ar.strip_tashkeel(tweet)\n    tweet = ar.strip_tatweel(tweet)\n    tweet = tweet.replace(\"@\", \" \")\n    tweet = tweet.replace(\"_\", \" \")\n    tweet = re.sub(\"ى\", \"ي\", tweet)\n    tweet = re.sub(\"ؤ\", \"ء\", tweet)\n    tweet = re.sub(\"ئ\", \"ء\", tweet)\n    tweet = re.sub(\"ة\", \"ه\", tweet)\n    tweet = re.sub(\"گ\", \"ك\", tweet)\n    tweet = tweet.replace(\"آ\", \"ا\")\n    tweet = tweet.replace(\"إ\", \"ا\")\n    tweet = tweet.replace(\"أ\", \"ا\")\n    tweet = tweet.replace(\"ؤ\", \"و\")\n    tweet = tweet.replace(\"ئ\", \"ي\")\n    tweet = nltk.tokenize.word_tokenize(tweet)\n    tweet = [ISRIStemmer().suf32(w) for w in tweet]\n    for i in range(len(tweet)):\n        if tweet[i] == 'اه':\n            tweet[i] = 'الله'\n    return tweet\ncontractions_fr = {\n        'administration':'admin',\n        'avec':'ac',\n        'beaucoup':'bp',\n        'c’est-à-dire':'cad',\n        'cependant':'cpd',\n        'chose':'ch',\n        'conclusion':'ccl',\n        'confer ':'cf',\n        'court terme':'ct',\n        'dans':'ds',\n        'dedans':'dd',\n        'définition':'déf',\n        'et cetera':'etc',\n        'être':'ê',\n        'exemple':'ex',\n        'extérieur':'ext',\n        'font':'ft',\n        'général':'gal',\n        'gouvernement':'gouv',\n        'grand':'gd',\n        'groupe':'gp',\n        'identique':'idel',\n        'introduction':'intro',\n        'jour':'jr',\n        'long terme':'lt',\n        'lorsque':'lsq',\n        'mais':'ms',\n        'même':'^m',\n        'moyen terme':'mt',\n        'nombre':'nb',\n        'nombreux':'nbx',\n        'nombre':'nb',\n        'nombreux':'nbx',\n        'observation':'obs',\n        'ordre du jour':'oj',\n        'page':'p',\n        'parce que':'pcq',\n        'pendant':'pdt',\n        'personne':'pers',\n        'point':'pt',\n        'peut-être':'pê',\n        'pour':'pr',\n        'pourtant':'prtt',\n        'quand':'qd',\n        'quantité':'qté',\n        'que':'q',\n        'quelqu’un':'qqn',\n        'quelque chose':'qqch',\n        'quelque':'qq',\n        'quelquefois':'qqf',\n        'question':'quest',\n        'rendez-vous':'rdv',\n        'responsabilité':'respité',\n        'seulement':'slt',\n        'solution':'sol',\n        'sont':'st',\n        'sous':'ss',\n        'souvent':'svt',\n        'temps':'tps',\n        'toujours':'tjrs',\n        'tous':'ts',\n        'tout':'tt',\n        'toute':'tte',\n        'toutes':'ttes',\n        'vous':'vs',\n        'le':'l\\'',\n        'me':'m\\'',\n        'de':'d\\'',\n        'te':'t\\'',\n        'se':'s\\'',\n        'ce':'c\\'',\n        'ne':'n\\'',\n        'que':'qu\\'',\n        'jusque':'jusqu\\'',\n        'lorsque':'lorsqu\\'',\n        'puisque':'puisqu\\'',\n        'quelque':'quelqu\\'',\n        'quoique':'quoiqu\\'',\n        'parce que':'parce qu\\'',\n        'tel que':'tel qu\\'',\n        'telle que':'telle qu\\'',       \n        'faculte':'fac',\n        'bien':'bin',    \n        'attend':'att',\n        'je': 'j\\'',\n        'rire':'ptdr',\n        'rire':'lol',\n        'rire':'lmfao',\n        'putin':'ptn',\n}\nfrench_stopwords = nltk.corpus.stopwords.words('french')\nlemmatizer = FrenchLefffLemmatizer()\ndef prepro_fr(tweet):\n    # prepare regex for char filtering\n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    # normalize unicode characters\n    tweet = normalize('NFD', tweet).encode('ascii','ignore')\n    tweet = tweet.decode('UTF-8')\n    #demojize\n    tweet = emoji.demojize(tweet)\n    if \"#\" in tweet:\n        tweet = hash_fix(tweet)\n    tweet = tweet.replace('user', '')\n    tweet = tweet.replace('@user', '')\n    tweet = tweet.replace(\"@\", \" \")\n    tweet = tweet.replace('url', '')\n    tweet = tweet.replace('rt', '')\n    tweet = re.sub( r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",'',tweet)\n    # convert to lower case\n    tweet = tweet.lower()\n    tweet = tweet.replace('\\'', '\\' ') \n    # remove punctuation\n    tweet = re.sub(r'\\u003c\\\\1', lambda m: \"-\" if m.group(0) == \"-\" else \"\", tweet)\n    # tokenization\n    tweet = nltk.tokenize.word_tokenize(tweet)\n    #contractions\n    tweet = [list(contractions_fr.keys())[list(contractions_fr.values()).index(word)] if word in contractions_fr.values() else word for word in tweet]\n    \n    # stop words\n    tweet = [w for w in tweet if w not in french_stopwords]\n    sc = 'j[a-z]*'\n    v = '[^aeyouisch]*'\n    for w in tweet:\n        x = re.findall(sc,w)\n        xx = re.findall(v,w)\n        if len(x)!=0:\n            w = x[0].replace('j', 'je ')\n            if len(x)>=3:\n                w = w.replace(xx[0], '')\n    # remove non-printable chars form each token\n    tweet = [re_print.sub('', w) for w in tweet]\n    # lemmatization\n    tweet = [lemmatizer.lemmatize(w) for w in tweet]\n    return tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ar.tweet = df_ar.tweet.apply(lambda t: prepro_ar(t))\ndf_fr.tweet = df_fr.tweet.apply(lambda t: prepro_fr(t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding labels\nle = preprocessing.LabelEncoder()\ndf_fr.sentiment = le.fit_transform(df_fr.sentiment)\ndf_fr.directness = le.fit_transform(df_fr.directness)\ndf_fr.target = le.fit_transform(df_fr.target)\ndf_fr.annotator_sentiment = le.fit_transform(df_fr.annotator_sentiment)\ndf_fr.group = le.fit_transform(df_fr.group)\ndf_fr = df_fr.sample(frac = 1)\n\n#encoding labels\nle = preprocessing.LabelEncoder()\ndf_ar.sentiment = le.fit_transform(df_ar.sentiment)\ndf_ar.directness = le.fit_transform(df_ar.directness)\ndf_ar.target = le.fit_transform(df_ar.target)\ndf_ar.annotator_sentiment = le.fit_transform(df_ar.annotator_sentiment)\ndf_ar.group = le.fit_transform(df_ar.group)\ndf_ar = df_ar.sample(frac = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fr,X_test_fr, y_train_fr,y_test_fr = train_test_split(df_fr.tweet,\n                                                              df_fr.target.values,\n                                                              test_size=0.1)\n\nX_train_ar,X_test_ar, y_train_ar,y_test_ar = train_test_split(df_ar.tweet,\n                                                              df_ar.target.values,\n                                                              test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = X_train_fr.to_frame()\ntrain_set.insert(0, \"target\", y_train_fr, True)\nfig,ax= plt.subplots(figsize=(12,5))\nax.set_xlabel('my x label', size = 13)\nax.set_ylabel('my y label', size = 13)\nax.set_xticklabels(\"\",size = 13)\nsn.countplot(x=\"target\", data=train_set, palette='PuRd_r',ax=ax)\nfig.suptitle('Training set distribution before resampling (French dataset) ', fontsize=10)\nplt.savefig('Training set distribution before resampling (French dataset)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_ar = X_train_ar.to_frame()\ntrain_set_ar.insert(0, \"target\", y_train_ar, True)\nfig,ax= plt.subplots(figsize=(12,5))\nax.set_xlabel('my x label', size = 13)\nax.set_ylabel('my y label', size = 13)\nax.set_xticklabels(\"\",size = 13)\nsn.countplot(x=\"target\", data=train_set_ar, palette='PuRd_r',ax=ax)\nfig.suptitle('Training set distribution before resampling (Arabic dataset) ', fontsize=10)\nplt.savefig('Training set distribution before resampling (Arabic dataset)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resampling\ndef resample_ar(X, y):\n    Xnew = X.copy()\n    ynew = y.copy()\n    setnew = Xnew\n    setnew.insert(0, \"target\", ynew, True)\n\n    lab0,lab1,lab2,lab3,lab4 = setnew[setnew.target==0],setnew[setnew.target==1],setnew[setnew.target==2],setnew[setnew.target==3],setnew[setnew.target==4]\n    c0,c1,c2,c3,c4 = setnew.target.value_counts()\n    av = int((c0+c1+c2+c3+c4)/5)  \n    lab0_sampled = lab0.sample(av, replace=True) \n    lab1_sampled = lab1.sample(av, replace=True)\n    lab2_sampled = lab2.sample(av, replace=True) \n    lab3_sampled = lab3.sample(av, replace=True) \n    lab4_sampled = lab4.sample(av, replace=True) \n    \n    setnew_resampled = pd.concat([lab0_sampled,lab1_sampled,lab2_sampled,lab3_sampled,lab4_sampled], axis=0)\n    setnew_resampled = setnew_resampled.sample(frac = 1)\n    ynew = setnew_resampled['target']\n    Xnew = setnew_resampled.drop(columns=['target'])\n    return Xnew, ynew\n\ndef resample_fr(X, y):\n    Xnew = X.copy()\n    ynew = y.copy()\n    setnew = Xnew\n    setnew.insert(0, \"target\", ynew, True)\n    lab0,lab1,lab2,lab3,lab4,lab5 = setnew[setnew.target==0],setnew[setnew.target==1],setnew[setnew.target==2],setnew[setnew.target==3],setnew[setnew.target==4],setnew[setnew.target==5]\n    c0,c1,c2,c3,c4,c5 = setnew.target.value_counts()\n    av = int((c0+c1+c2+c3+c4+c5)/6)  \n    lab0_sampled = lab0.sample(av, replace=True) \n    lab1_sampled = lab1.sample(av, replace=True)\n    lab2_sampled = lab2.sample(av, replace=True) \n    lab3_sampled = lab3.sample(av, replace=True) \n    lab4_sampled = lab4.sample(av, replace=True) \n    lab5_sampled = lab5.sample(av, replace=True) \n    setnew_resampled = pd.concat([lab0_sampled,lab1_sampled,lab2_sampled,lab3_sampled,lab4_sampled,lab5_sampled], axis=0)\n    setnew_resampled = setnew_resampled.sample(frac = 1)\n    ynew = setnew_resampled['target']\n    Xnew = setnew_resampled.drop(columns=['target'])\n    return Xnew, ynew\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fr_re,y_train_fr_re = resample_fr(X_train_fr.to_frame(),y_train_fr)\nX_train_ar_re,y_train_ar_re = resample_ar(X_train_ar.to_frame(),y_train_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic = {0:'disability', \n       1:'gender', \n       2:'origin', \n       3:'other', \n       4:'religion',\n       5:'sexual_orientation',}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = X_train_fr_re.copy()\ntrain_set.insert(0, \"target\", y_train_fr_re, True)\ntrain_set.target = np.array([dic[t] for t in y_train_fr_re])\nfig,ax= plt.subplots(figsize=(12,5))\nax.set_xlabel('', size = 13)\nax.set_ylabel('', size = 13)\nax.set_xticklabels(\"\",size = 13)\nsn.countplot(x=\"target\", data=train_set, palette='PuRd_r',ax=ax)\nfig.suptitle('Training set distribution after resampling (French dataset) ', fontsize=15)\nplt.savefig('Training set distribution after resampling (French dataset)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_ar = X_train_ar_re.copy()\ntrain_set_ar.insert(0, \"target\", y_train_ar_re, True)\ntrain_set_ar.target = np.array([dic[t] for t in y_train_ar_re])\nfig,ax= plt.subplots(figsize=(12,5))\nax.set_xlabel('', size = 13)\nax.set_ylabel('', size = 13)\nax.set_xticklabels(\"\",size = 13)\nsn.countplot(x=\"target\", data=train_set_ar, palette='PuRd_r',ax=ax)\nfig.suptitle('Training set distribution after resampling (Arabic dataset) ', fontsize=15)\nplt.savefig('Training set distribution after resampling (Arabic dataset)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights_train_fr = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(y_train_fr),\n                                        y = y_train_fr                                                    \n                                    )\nclass_weights_train_fr = dict(zip(np.unique(y_train_fr), class_weights_train_fr))\n\nprint('Class weights for french dataset: '+'\\n',class_weights_train_fr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights_train_ar = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(y_train_ar),\n                                        y = y_train_ar                                                    \n                                    )\nclass_weights_train_ar = dict(zip(np.unique(y_train_ar), class_weights_train_ar))\n\nprint('Class weights for Arabic dataset: '+'\\n',class_weights_train_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_fr, y_test_fr = pd.get_dummies(y_train_fr), pd.get_dummies(y_test_fr)\ny_train_ar, y_test_ar = pd.get_dummies(y_train_ar), pd.get_dummies(y_test_ar)\n\ny_train_fr_re = pd.get_dummies(y_train_fr_re)\ny_train_ar_re = pd.get_dummies(y_train_ar_re)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding","metadata":{}},{"cell_type":"code","source":"emd_vect =  300\ndef embedding_mat(file, word_index):\n    embedding_dict = {}\n    for line in file:\n        values=line.split()\n        word=values[0]\n        if word in word_index.keys():\n            vector = np.asarray(values[1:], 'float32')\n            embedding_dict[word] = vector\n\n    num_words=len(word_index)+1\n    embedding_matrix=np.zeros((num_words, emd_vect))\n    for word,i in tqdm(word_index.items()):\n        vect=embedding_dict.get(word)\n        if vect is not None:\n            embedding_matrix[i]  = vect[:emd_vect]\n    return embedding_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_fr = open('/kaggle/input/vectorization/cc.fr.300.vec', encoding='utf-8')\nf_ar = open('/kaggle/input/vectorization-ar/cc.ar.300.vec', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fr = list(X_train_fr)\nX_test_fr = list(X_test_fr)\n\nX_train_ar = list(X_train_ar)\nX_test_ar = list(X_test_ar)\n\n\ntok_fr = tf.keras.preprocessing.text.Tokenizer()\ntok_fr.fit_on_texts(df_fr.tweet.values)\n#text to integer sequence\nX_train_fr = tok_fr.texts_to_sequences(X_train_fr)\nX_test_fr = tok_fr.texts_to_sequences(X_test_fr)\n\ntok_ar = tf.keras.preprocessing.text.Tokenizer()\ntok_ar.fit_on_texts(df_ar.tweet.values)\nword_index_ar = tok_ar.word_index\n\n#text to integer sequence\nX_train_ar = tok_ar.texts_to_sequences(X_train_ar)\nX_test_ar = tok_ar.texts_to_sequences(X_test_ar)\n\n\n#counting the length of the tweet and taking the max\ntweets_length_fr= [len(X_train_fr[i]) for i in range(len(X_train_fr))]\nmax_seq_len_fr = max(tweets_length_fr)\nprint(\"French max sequence length\",max_seq_len_fr)\n#counting the length of the tweet and taking the max\ntweets_length_ar= [len(X_train_ar[i]) for i in range(len(X_train_ar))]\nmax_seq_len_ar = max(tweets_length_ar)\nprint(\"Arabic max sequence length\",max_seq_len_ar)\n\n\n#padding\nX_train_fr1 = pad_sequences(X_train_fr, maxlen=max_seq_len_fr)\nX_test_fr1 = pad_sequences(X_test_fr, maxlen=max_seq_len_fr)\n#X_train_fr2 = X_train_fr[['sentiment', 'directness', 'group']].values\n#X_test_fr2 = X_test_fr[['sentiment', 'directness', 'group']].values\nembedding_matrix_fr = embedding_mat(f_fr, tok_fr.word_index)\nembedding_matrix_fr\n\n#padding\nX_train_ar1 = pad_sequences(X_train_ar, maxlen=max_seq_len_ar)\nX_test_ar1 = pad_sequences(X_test_ar, maxlen=max_seq_len_ar)\n#X_train_ar2 = X_train_ar[['sentiment', 'directness', 'annotator_sentiment', 'group']].values\n#X_test_ar2 = X_test_ar[['sentiment', 'directness', 'annotator_sentiment', 'group']].values\nembedding_matrix_ar = embedding_mat(f_ar, tok_ar.word_index)\nembedding_matrix_ar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_fr = open('/kaggle/input/vectorization/cc.fr.300.vec', encoding='utf-8')\nf_ar = open('/kaggle/input/vectorization-ar/cc.ar.300.vec', encoding='utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_fr_re = list(X_train_fr_re.tweet.values)\nX_train_ar_re = list(X_train_ar_re.tweet.values)\n\ntok_fr = tf.keras.preprocessing.text.Tokenizer()\ntok_fr.fit_on_texts(df_fr.tweet.values)\n#text to integer sequence\nX_train_fr_re = tok_fr.texts_to_sequences(X_train_fr_re)\n\n#text to integer sequence\nX_train_ar_re = tok_ar.texts_to_sequences(X_train_ar_re)\n\n\n#counting the length of the tweet and taking the max\ntweets_length_fr_re= [len(X_train_fr_re[i]) for i in range(len(X_train_fr_re))]\nmax_seq_len_fr_re = max(tweets_length_fr_re)\nprint(\"French max sequence length\",max_seq_len_fr_re)\n\ntok_ar = tf.keras.preprocessing.text.Tokenizer()\ntok_ar.fit_on_texts(df_ar.tweet.values)\n#counting the length of the tweet and taking the max\ntweets_length_ar_re= [len(X_train_ar_re[i]) for i in range(len(X_train_ar_re))]\nmax_seq_len_ar_re = max(tweets_length_ar_re)\nprint(\"Arabic max sequence length\",max_seq_len_ar_re)\n\n\n#padding\nX_train_fr1_re = pad_sequences(X_train_fr_re, maxlen=max_seq_len_fr_re)\n#X_train_fr2_re = X_train_fr_re[['sentiment', 'directness', 'group']].values\nembedding_matrix_fr_re = embedding_mat(f_fr, tok_fr.word_index)\nembedding_matrix_fr_re\n\n#padding\nX_train_ar1_re = pad_sequences(X_train_ar_re, maxlen=max_seq_len_ar_re)\n#X_train_ar2 = X_train_ar_re[['sentiment', 'directness', 'annotator_sentiment', 'group']].values\nembedding_matrix_ar_re = embedding_mat(f_ar, tok_ar.word_index)\nembedding_matrix_ar_re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification models","metadata":{}},{"cell_type":"code","source":"input_fr_1 = Input(shape=(max_seq_len_fr,))\nnb_words_fr = len(tok_fr.word_index)+1\nnb_classes_fr = 6\n\ninput_ar_1 = Input(shape=(max_seq_len_ar,))\nnb_words_ar = len(tok_ar.word_index)+1\nnb_classes_ar = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MODEL:\n    def __init__(self,embedding_matrix,nb_classes,nb_words,out_dim = 300):\n            self.embedding_matrix = embedding_matrix\n            self.nb_classes = nb_classes\n            self.nb_words = nb_words\n            self.out_dim = out_dim\n    \n    def model(self):\n        model = tf.keras.Sequential()\n        embedding_layer = tf.keras.layers.Embedding(\n                                self.nb_words, \n                                self.out_dim,  \n                                weights=[self.embedding_matrix],\n                                trainable=False,\n                            )\n        model.add(embedding_layer)\n        model.add(tf.keras.layers.Bidirectional(GRU(128)))\n        model.add(Dense(100,activation= 'relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(32, activation='relu'))\n        model.add(Dropout(0.4))\n        model.add(Dense(self.nb_classes, activation='softmax'))\n\n        return model\n        \n    def train(self,model,num_epochs,batch_size,X,y):\n        early = callbacks.EarlyStopping(monitor='val_loss',\n                            min_delta=0, \n                            patience=3,\n                            verbose=1, \n                            mode='auto')\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy']\n                     )\n        \n        history = model.fit(X,\n                      y,\n                      batch_size=batch_size,\n                      epochs=num_epochs,\n                      validation_split=0.25,\n                      callbacks = [early],\n                      verbose=1)\n        return history\n    \n    \nclass MODEL_WE:\n    def __init__(self,class_weights,embedding_matrix,nb_classes,nb_words,out_dim = 300):\n            self.class_weights = class_weights\n            self.embedding_matrix = embedding_matrix\n            self.nb_classes = nb_classes\n            self.nb_words = nb_words\n            self.out_dim = out_dim\n    \n    def model(self):\n        model = tf.keras.Sequential()\n        embedding_layer = tf.keras.layers.Embedding(\n                                self.nb_words, \n                                self.out_dim,  \n                                weights=[self.embedding_matrix],\n                                trainable=False,\n                            )\n        model.add(embedding_layer)\n        model.add(tf.keras.layers.Bidirectional(LSTM(300)))\n        model.add(Dense(600,activation= 'relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(150, activation='relu'))\n        model.add(Dropout(0.3))\n        model.add(Dense(50, activation='relu'))\n        model.add(Dropout(0.4))\n        model.add(Dense(self.nb_classes, activation='softmax'))\n\n        return model\n        \n    def train(self,model,num_epochs,batch_size,X,y):\n        early = callbacks.EarlyStopping(monitor='val_loss',\n                            min_delta=0, \n                            patience=3,\n                            verbose=1, \n                            mode='auto')\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy']\n                     )\n        \n        history = model.fit(X,\n                      y,\n                      batch_size=batch_size,\n                      epochs=num_epochs,\n                      validation_split=0.25,\n                      callbacks = [early],\n                      class_weight=self.class_weights,\n                      verbose=1)\n        return history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_acc_loss(history):\n    \"\"\"\n    Plot accuracy and loss of a model\n    @params:\n            - history: history of the model\n    @return:\n            plots\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(10,5))\n    l = list(history.history.keys())\n    # accuracy plot\n    ax[0].plot(history.history[l[1]])\n    ax[0].plot(history.history[l[3]])\n    ax[0].set_title('model accuracy')\n    ax[0].set_ylabel('accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].legend(['train', 'val'], loc='upper left')\n    # loss plot\n    ax[1].plot(history.history[l[0]])\n    ax[1].plot(history.history[l[2]])\n    ax[1].set_title('model loss')\n    ax[1].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].legend(['train', 'val'], loc='upper left')\n    fig.suptitle('Model results for resampled dataset')\n    plt.savefig(\"Model results for resampled dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_fr(model):\n    pred = model.predict(X_test_fr1)\n    pred_fr = [np.argmax(i) for i in pred]\n    gt = y_test_fr.idxmax(axis=1).values\n    # precision tp / (tp + fp)\n    precision_fr = precision_score(list(gt), pred_fr,average='macro')\n    # accuracy: (tp + tn) / (p + n)\n    accuracy_fr = accuracy_score(list(gt), pred_fr)\n    # recall: tp / (tp + fn)\n    recall_fr = recall_score(list(gt), pred_fr, labels=[0,1,2,3,4,5],average='macro')\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1_fr = f1_score(list(gt), pred_fr,labels=[0,1,2,3,4,5],average='weighted')\n    return precision_fr,accuracy_fr,recall_fr,f1_fr\n\ndef eval_ar(model):\n    pred_ar = model.predict(X_test_ar1)\n    pred_ar = [np.argmax(i) for i in pred_ar]\n    gt_ar = y_test_ar.idxmax(axis=1).values\n    # precision tp / (tp + fp)\n    precision_ar = precision_score(list(gt_ar), pred_ar,average='macro')\n    # accuracy: (tp + tn) / (p + n)\n    accuracy_ar = accuracy_score(list(gt_ar), pred_ar)\n    # recall: tp / (tp + fn)\n    recall_ar = recall_score(list(gt_ar), pred_ar, labels=[0,1,2,3,4],average='macro')\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1_ar = f1_score(list(gt_ar), pred_ar,labels=[0,1,2,3,4],average='weighted')\n    return precision_ar,accuracy_ar,recall_ar,f1_ar\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL WITHOUT WEIGHTS\nmod = MODEL(embedding_matrix_fr,nb_classes_fr,nb_words_fr)\nmodel = mod.model()\nhis = mod.train(model,30,32,X_train_fr1_re,y_train_fr_re)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(his)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_fr,acc_fr,rec_fr,f1_fr = eval_fr(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_ar = MODEL(embedding_matrix_ar,nb_classes_ar,nb_words_ar)\nmodel_ar = mod_ar.model()\nhis_ar = mod_ar.train(model_ar,30,32,X_train_ar1_re,y_train_ar_re)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(his_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_ar,acc_ar,rec_ar,f1_ar = eval_ar(model_ar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_ar,acc_ar,rec_ar,f1_ar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL WITH WEIGHTS\nmod_we = MODEL_WE(class_weights_train_fr,embedding_matrix_fr,nb_classes_fr,nb_words_fr)\nmodel_we = mod_we.model()\nhis_we = mod_we.train(model,30,32,X_train_fr1_re,y_train_fr_re)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(his_we)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_fr_we,acc_fr_we,rec_fr_we,f1_fr_we = eval_fr(model_we)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_fr_we,acc_fr_we,rec_fr_we,f1_fr_we","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_ar_we = MODEL_WE(class_weights_train_ar,embedding_matrix_ar,nb_classes_ar,nb_words_ar)\nmodel_ar_we = mod_ar_we.model()\nhis_ar_we = mod_ar_we.train(model_ar_we,30,32,X_train_ar1_re,y_train_ar_re)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(his_ar_we)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_ar_we,acc_ar_we,rec_ar_we,f1_ar_we = eval_ar(model_ar_we)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing on new data","metadata":{}},{"cell_type":"code","source":"def predicting(tweet,model):\n    x = pd.DataFrame({\"tweet\":[tweet]})\n    p = [prepro_fr(t) for t in x.tweet]\n    x = tok_fr.texts_to_sequences(p)\n    x = pad_sequences(x, maxlen=max_seq_len_fr)\n    pred = np.argmax(model.predict(x))\n    if pred == 0:\n        return 'disability'\n    elif pred==1:\n        return 'gender'\n    elif pred == 2:\n        return 'origin'\n    elif pred==3:\n        return 'other'\n    elif pred == 4:\n        'religion'\n    else:\n        return'sexual_orientation'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}